#!/usr/bin/env python3
"""
ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawlers.robust_crawler import RobustWebCrawler

def test_robust_crawler():
    """ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ê°•í™”ëœ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = RobustWebCrawler(rate_limit=1.0, timeout=30)
        print("âœ… ê°•í™”ëœ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì„±ê³µ!")
        
        # í…ŒìŠ¤íŠ¸ URL
        test_url = "https://docs.gelatto.ai/"
        print(f"í…ŒìŠ¤íŠ¸ URL: {test_url}")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        print("\ní¬ë¡¤ë§ ì‹¤í–‰ ì¤‘...")
        pages = crawler.crawl(test_url, max_pages=5)
        
        print(f"\ní¬ë¡¤ë§ ê²°ê³¼: {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬")
        
        if pages:
            print("\ní˜ì´ì§€ ì •ë³´:")
            for i, page in enumerate(pages):
                print(f"  {i+1}. {page['title']}")
                print(f"     URL: {page['url']}")
                print(f"     ë‚´ìš© ê¸¸ì´: {len(page['content'])}ì")
                print(f"     ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {page['content'][:100]}...")
                print()
        else:
            print("âŒ í˜ì´ì§€ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_robust_crawler()
    if success:
        print("\nğŸ‰ ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nğŸ’¥ ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        sys.exit(1)
