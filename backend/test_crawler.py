#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawlers.help_doc_crawler import HelpDocCrawler
import requests
from bs4 import BeautifulSoup

def test_crawler():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ë¨¼ì € ì§ì ‘ ì›¹í˜ì´ì§€ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        test_url = "https://docs.gelatto.ai/"
        print(f"ì§ì ‘ ì›¹í˜ì´ì§€ ì ‘ê·¼ í…ŒìŠ¤íŠ¸: {test_url}")
        
        response = requests.get(test_url, timeout=10)
        print(f"HTTP ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"í˜ì´ì§€ ì œëª©: {soup.title.string if soup.title else 'N/A'}")
            
            # ëª¨ë“  ë§í¬ ì°¾ê¸°
            all_links = soup.find_all('a', href=True)
            print(f"ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}")
            
            # ì²˜ìŒ 10ê°œ ë§í¬ ì¶œë ¥
            for i, link in enumerate(all_links[:10]):
                href = link.get('href')
                title = link.get_text(strip=True)
                print(f"  {i+1}. {title} -> {href}")
        
        print("\n" + "="*50)
        print("í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = HelpDocCrawler(rate_limit=1.0, timeout=30, max_pages=5)
        print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì„±ê³µ!")
        
        # _find_feature_links ë©”ì„œë“œ ì§ì ‘ í…ŒìŠ¤íŠ¸
        print("\nê¸°ëŠ¥ ë§í¬ ì°¾ê¸° í…ŒìŠ¤íŠ¸...")
        feature_links = crawler._find_feature_links(soup, test_url)
        print(f"ë°œê²¬ëœ ê¸°ëŠ¥ ë§í¬ ìˆ˜: {len(feature_links)}")
        
        for i, link in enumerate(feature_links[:5]):
            print(f"  {i+1}. {link['title']} -> {link['url']}")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        pages = crawler.crawl(test_url)
        
        print(f"\ní¬ë¡¤ë§ ê²°ê³¼: {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬")
        
        if pages:
            print("ì²« ë²ˆì§¸ í˜ì´ì§€ ë‚´ìš©:")
            print(f"ì œëª©: {pages[0].get('title', 'N/A')}")
            print(f"URL: {pages[0].get('url', 'N/A')}")
            print(f"ë‚´ìš© ê¸¸ì´: {len(pages[0].get('content', ''))}ì")
            print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {pages[0].get('content', '')[:200]}...")
        else:
            print("âŒ í˜ì´ì§€ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_crawler()
    if success:
        print("\nğŸ‰ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nğŸ’¥ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        sys.exit(1)
