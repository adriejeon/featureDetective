#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ë¬¸ì œ ë””ë²„ê¹… ë„êµ¬
"""

import requests
import chardet
from bs4 import BeautifulSoup
import sys

def debug_crawling_issue(url: str):
    """í¬ë¡¤ë§ ë¬¸ì œ ë””ë²„ê¹…"""
    print(f"ðŸ” URL ë””ë²„ê¹…: {url}")
    print("="*50)
    
    try:
        # 1. ê¸°ë³¸ ìš”ì²­
        print("1ï¸âƒ£ HTTP ìš”ì²­ ì‹œë„...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        print(f"   âœ… HTTP ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"   ðŸ“„ ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
        
        # 2. ì¸ì½”ë”© ë¶„ì„
        print("\n2ï¸âƒ£ ì¸ì½”ë”© ë¶„ì„...")
        print(f"   ðŸ·ï¸  ì„œë²„ ì¸ì½”ë”©: {response.encoding}")
        
        # chardetë¡œ ì¸ì½”ë”© ê°ì§€
        detected = chardet.detect(response.content)
        print(f"   ðŸ” ê°ì§€ëœ ì¸ì½”ë”©: {detected}")
        
        # 3. ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        print("\n3ï¸âƒ£ ì¸ì½”ë”©ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        
        encodings_to_try = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1', 'ascii']
        
        for encoding in encodings_to_try:
            try:
                text = response.content.decode(encoding, errors='ignore')
                print(f"   âœ… {encoding}: {len(text)}ìž (ì²˜ìŒ 100ìž: {text[:100].replace(chr(10), ' ').replace(chr(13), ' ')})")
            except Exception as e:
                print(f"   âŒ {encoding}: ì‹¤íŒ¨ - {e}")
        
        # 4. BeautifulSoup íŒŒì‹± í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ BeautifulSoup íŒŒì‹± í…ŒìŠ¤íŠ¸...")
        
        # UTF-8ë¡œ ì‹œë„
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
            print(f"   ðŸ“„ íŽ˜ì´ì§€ ì œëª©: {title}")
            
            # ë§í¬ ìˆ˜ í™•ì¸
            links = soup.find_all('a', href=True)
            print(f"   ðŸ”— ë§í¬ ìˆ˜: {len(links)}")
            
            # ì²˜ìŒ 5ê°œ ë§í¬ ì¶œë ¥
            for i, link in enumerate(links[:5]):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                print(f"      {i+1}. {text} -> {href}")
                
        except Exception as e:
            print(f"   âŒ BeautifulSoup íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # 5. í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„
        print("\n5ï¸âƒ£ í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„...")
        try:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for element in soup(['script', 'style', 'nav', 'footer', 'header']):
                element.decompose()
            
            text = soup.get_text()
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            print(f"   ðŸ“ í…ìŠ¤íŠ¸ ë¼ì¸ ìˆ˜: {len(lines)}")
            print(f"   ðŸ“ ì²« 5ì¤„:")
            for i, line in enumerate(lines[:5]):
                print(f"      {i+1}. {line}")
                
        except Exception as e:
            print(f"   âŒ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*50)
        print("âœ… ë””ë²„ê¹… ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë””ë²„ê¹… ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = "https://docs.gelatto.ai/"
    
    debug_crawling_issue(url)

